<configuration>
        <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->
        <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
        </property>
        <!-- mycluster下面有两个NameNode，分别是 node-191,node-192 -->
        <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>node-191,node-192</value>
        </property>
        <!-- service1的RPC通信地址 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.node-191</name>
                <value>node-191:9000</value>
        </property>
        <!-- service1的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.node-191</name>
                <value>node-191:50070</value>
        </property>
        <!-- service6的RPC通信地址 -->
        <property>
                <name>dfs.namenode.rpc-address.mycluster.node-192</name>
                <value>node-192:9000</value>
        </property>
        <!-- service6的http通信地址 -->
        <property>
                <name>dfs.namenode.http-address.mycluster.node-192</name>
                <value>node-192:50070</value>
        </property>
        <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://node-196:8485;node-197:8485;node-198:8485/mycluster</value>
        </property>
        <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/opt/hadoop/journal</value>
        </property>
        <!-- 开启NameNode失败自动切换 -->
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
        <!-- 配置失败自动切换实现方式 -->
        <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>
                        sshfence
                        shell(/bin/true)
                </value>
        </property>
        <!-- 使用sshfence隔离机制时需要ssh免登陆 -->
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/root/.ssh/id_rsa</value>
        </property>
        <!-- 配置sshfence隔离机制超时时间 -->
        <property>
                <name>dfs.ha.fencing.ssh.connect-timeout</name>
                <value>30000</value>
        </property>
        <!-- datanode 白名单 -->
        <property>
                <name>dfs.hosts</name>
                <value>/usr/local/hadoop-2.7.4/etc/hadoop/slaves</value>
        </property>
        <property>
               <name>dfs.datanode.data.dir</name>
               <value>file:///opt/hadoop/datanode</value>
        </property>
        <property>
               <name>dfs.webhdfs.enabled</name>
               <value>true</value>
        </property>
</configuration>
